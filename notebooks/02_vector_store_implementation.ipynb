{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8613960-9b43-45c3-b0a1-2042e6cbed59",
   "metadata": {},
   "source": [
    "# Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1dd0c4fe-174c-4f32-ac90-5713b9ef8b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete. Database path: ../data/processed/chroma_db\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define where we will save the database on disk\n",
    "# We use the 'data/processed' folder we created earlier\n",
    "PERSIST_DIRECTORY = \"../data/processed/chroma_db\"\n",
    "\n",
    "# Setup the embedding function (The Translator)\n",
    "# We use the same model as before: 'all-MiniLM-L6-v2'\n",
    "embedding_func = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name=\"all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "print(\"Setup complete. Database path:\", PERSIST_DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774ce1f9-eaaa-44b1-8244-043d6ad25eb7",
   "metadata": {},
   "source": [
    "# Initialize Client & Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "607c6837-dea0-4ca5-bd07-d9548b3055da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection 'customer_feedback' is ready.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the ChromaDB client (Persistent means it saves to disk)\n",
    "client = chromadb.PersistentClient(path=PERSIST_DIRECTORY)\n",
    "\n",
    "# Create (or get if it already exists) a collection\n",
    "collection = client.get_or_create_collection(\n",
    "    name=\"customer_feedback\",\n",
    "    embedding_function=embedding_func,  # Chroma will handle vectorization automatically!\n",
    "    metadata={\"hnsw:space\": \"cosine\"},  # Use Cosine Similarity\n",
    ")\n",
    "\n",
    "print(f\"Collection '{collection.name}' is ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14804f3a-55ec-4ae6-9e18-279116f2c747",
   "metadata": {},
   "source": [
    "# Adding Data (Ingestion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c219d99-236c-4c0e-af81-6448c405d0b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Scanning directory: /home/guga/Work/rag-feedback-analyzer/data/raw\n",
      "âœ… Found file: comcast_fcc_complaints_2015.csv\n",
      "ðŸ“Š Loaded 2225 rows.\n",
      "ðŸŽ¯ Using text column: 'customer_complaint'\n",
      "ðŸ“ Prepared 1000 documents for ingestion.\n",
      "ðŸ” Current Vector Store Status: 7 documents.\n",
      "ðŸ§¹ Detecting legacy/test data. Purging collection to ensure a clean slate...\n",
      "âœ… Collection cleared. Current count: 0\n",
      "ðŸš€ Ingesting 1000 real Comcast documents into ChromaDB...\n",
      "   (This process calculates embeddings and might take 1-2 minutes on CPU...)\n",
      "--------------------------------------------------\n",
      "ðŸŽ‰ SUCCESS! Vector Store successfully populated.\n",
      "ðŸ“Š Total Real Documents: 1000\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# Professional Path Handling using pathlib\n",
    "PROJECT_ROOT = Path(\"..\")\n",
    "RAW_DATA_DIR = PROJECT_ROOT / \"data\" / \"raw\"\n",
    "\n",
    "print(f\"ðŸ“‚ Scanning directory: {RAW_DATA_DIR.resolve()}\")\n",
    "\n",
    "# 1. Auto-Discovery Logic\n",
    "# Instead of hardcoding a filename, we find any .csv file in the folder.\n",
    "# This makes the pipeline robust to filename changes.\n",
    "csv_files = list(RAW_DATA_DIR.glob(\"*.csv\"))\n",
    "\n",
    "if not csv_files:\n",
    "    print(\"\\nâŒ ERROR: No CSV file found in 'data/raw/'\")\n",
    "    print(\"ðŸ‘‰ ACTION REQUIRED: Download the CSV manually and place it in that folder.\")\n",
    "    # Stop execution gracefully\n",
    "    raise FileNotFoundError(\"Missing raw data file.\")\n",
    "\n",
    "target_file = csv_files[0]\n",
    "print(f\"âœ… Found file: {target_file.name}\")\n",
    "\n",
    "# 2. Load Data\n",
    "try:\n",
    "    df = pd.read_csv(target_file)\n",
    "    print(f\"ðŸ“Š Loaded {len(df)} rows.\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error reading CSV: {e}\")\n",
    "    raise e\n",
    "\n",
    "# 3. Data Cleaning Pipeline\n",
    "# Standardize column headers (snake_case)\n",
    "df.columns = (\n",
    "    df.columns.str.lower().str.strip().str.replace(\" \", \"_\").str.replace(\"-\", \"_\")\n",
    ")\n",
    "\n",
    "# Dynamic Text Column Detection\n",
    "# The code looks for common names for the complaint column\n",
    "possible_cols = [\"customer_complaint\", \"consumer_complaint_narrative\", \"complaint\"]\n",
    "text_col = next((c for c in possible_cols if c in df.columns), None)\n",
    "\n",
    "if not text_col:\n",
    "    raise ValueError(\n",
    "        f\"Could not find a complaint text column. Available columns: {df.columns.tolist()}\"\n",
    "    )\n",
    "\n",
    "print(f\"ðŸŽ¯ Using text column: '{text_col}'\")\n",
    "\n",
    "# Remove empty complaints\n",
    "df = df.dropna(subset=[text_col])\n",
    "\n",
    "# OPTIONAL: Sampling for Local Development\n",
    "# If the dataset is huge, we sample 1000 rows to keep the demo snappy on CPU.\n",
    "# In production, you would remove this limit.\n",
    "if len(df) > 1000:\n",
    "    df_subset = df.sample(1000, random_state=42).copy()\n",
    "else:\n",
    "    df_subset = df.copy()\n",
    "\n",
    "# 4. Prepare Embeddings\n",
    "documents = df_subset[text_col].tolist()\n",
    "\n",
    "metadatas = []\n",
    "for index, row in df_subset.iterrows():\n",
    "    # Defensive programming: Handle missing metadata gracefully\n",
    "    status = str(row.get(\"status\", \"Unknown\"))\n",
    "    ticket = str(row.get(\"ticket_#\", row.get(\"ticket_id\", \"Unknown\")))\n",
    "\n",
    "    metadatas.append(\n",
    "        {\"status\": status, \"ticket_id\": ticket, \"source\": \"Manual Download\"}\n",
    "    )\n",
    "\n",
    "# ... (Keep the previous data loading and cleaning logic unchanged) ...\n",
    "\n",
    "ids = [f\"ticket_{i}\" for i in range(len(documents))]\n",
    "\n",
    "print(f\"ðŸ“ Prepared {len(documents)} documents for ingestion.\")\n",
    "\n",
    "# --- DATABASE RESET & INGESTION LOGIC ---\n",
    "\n",
    "# 1. Check current database state\n",
    "current_count = collection.count()\n",
    "print(f\"ðŸ” Current Vector Store Status: {current_count} documents.\")\n",
    "\n",
    "# 2. Reset: Purge old test data (the 7 dummy records) if they exist\n",
    "if current_count > 0:\n",
    "    print(\n",
    "        \"ðŸ§¹ Detecting legacy/test data. Purging collection to ensure a clean slate...\"\n",
    "    )\n",
    "\n",
    "    # Retrieve all existing IDs to delete them\n",
    "    existing_data = collection.get()\n",
    "    existing_ids = existing_data[\"ids\"]\n",
    "\n",
    "    if existing_ids:\n",
    "        collection.delete(ids=existing_ids)\n",
    "\n",
    "    print(f\"âœ… Collection cleared. Current count: {collection.count()}\")\n",
    "\n",
    "# 3. Ingest Real Data\n",
    "print(f\"ðŸš€ Ingesting {len(documents)} real Comcast documents into ChromaDB...\")\n",
    "print(\"   (This process calculates embeddings and might take 1-2 minutes on CPU...)\")\n",
    "\n",
    "collection.add(documents=documents, ids=ids, metadatas=metadatas)\n",
    "\n",
    "# 4. Final Verification\n",
    "final_count = collection.count()\n",
    "print(\"-\" * 50)\n",
    "print(f\"ðŸŽ‰ SUCCESS! Vector Store successfully populated.\")\n",
    "print(f\"ðŸ“Š Total Real Documents: {final_count}\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbeeff20-8b1e-4de1-b66b-ac741d1def9e",
   "metadata": {},
   "source": [
    "# Semantic Search (The Query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6fcc8a3c-84c7-4dd9-b45f-b0907a31c2a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”Ž Searching for: 'internet speed is extremely slow and connection drops'\n",
      "\n",
      "ðŸ“„ Result 1:\n",
      "   Ticket ID: 360349\n",
      "   Status:    Pending\n",
      "   Complaint: Slow Internet Speeds...\n",
      "--------------------------------------------------\n",
      "ðŸ“„ Result 2:\n",
      "   Ticket ID: 364377\n",
      "   Status:    Open\n",
      "   Complaint: Slow Internet speeds...\n",
      "--------------------------------------------------\n",
      "ðŸ“„ Result 3:\n",
      "   Ticket ID: 224912\n",
      "   Status:    Closed\n",
      "   Complaint: Slow internet speeds...\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- SEMANTIC SEARCH VALIDATION ---\n",
    "\n",
    "# Now that we have real data, let's test the \"Brain\" of the system.\n",
    "# We will search for a concept, not necessarily exact keywords.\n",
    "\n",
    "# 1. Define a query relevant to the Telecom domain\n",
    "query_text = \"internet speed is extremely slow and connection drops\"\n",
    "\n",
    "print(f\"ðŸ”Ž Searching for: '{query_text}'\\n\")\n",
    "\n",
    "# 2. Query the Vector Database\n",
    "results = collection.query(\n",
    "    query_texts=[query_text],\n",
    "    n_results=3,  # Retrieve top 3 most similar complaints\n",
    ")\n",
    "\n",
    "# 3. Display Results\n",
    "for i, doc in enumerate(results[\"documents\"][0]):\n",
    "    meta = results[\"metadatas\"][0][i]\n",
    "    distance = results[\"distances\"][0][i] if \"distances\" in results else \"N/A\"\n",
    "\n",
    "    print(f\"ðŸ“„ Result {i + 1}:\")\n",
    "    print(f\"   Ticket ID: {meta['ticket_id']}\")\n",
    "    print(f\"   Status:    {meta['status']}\")\n",
    "    print(f\"   Complaint: {doc[:200]}...\")  # Show first 200 chars to keep it clean\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cafccd5-211a-44a0-8c94-e1c91e251b3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (RAG Project)",
   "language": "python",
   "name": "rag-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
